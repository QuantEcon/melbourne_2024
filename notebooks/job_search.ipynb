{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad94f101",
   "metadata": {},
   "source": [
    "# Job Search\n",
    "\n",
    "----\n",
    "#### John Stachurski (August 2024)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462763ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11079ae",
   "metadata": {},
   "source": [
    "In this lecture we study a basic infinite-horizon job search problem with Markov wage\n",
    "draws \n",
    "\n",
    "The exercise at the end asks you to add recursive preferences and compare\n",
    "the result.\n",
    "\n",
    "We use the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb00435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import quantecon as qe\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from collections import namedtuple\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c2ece",
   "metadata": {},
   "source": [
    "Let's check our GPU status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286276dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bfda2",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We study an elementary model where \n",
    "\n",
    "* jobs are permanent \n",
    "* unemployed workers receive current compensation $c$\n",
    "* the wage offer distribution $\\{W_t\\}$ is Markovian\n",
    "* the horizon is infinite\n",
    "* an unemployment agent discounts the future via discount factor $\\beta \\in (0,1)$\n",
    "\n",
    "\n",
    "An unemployed worker tries to maximize an expected sum of discounted lifetime payoffs.\n",
    "\n",
    "### Set up\n",
    "\n",
    "The wage offer process obeys\n",
    "\n",
    "$$\n",
    "    W_{t+1} = \\rho W_t + \\nu Z_{t+1}\n",
    "$$\n",
    "\n",
    "where $(Z_t)_{t \\geq 0}$ is IID and standard normal.\n",
    "\n",
    "We discretize this wage process using Tauchen's method to produce a stochastic matrix $P$\n",
    "\n",
    "Since jobs are permanent, the return to accepting wage offer $w$ today is\n",
    "\n",
    "$$\n",
    "    w + \\beta w + \\beta^2 w + \\frac{w}{1-\\beta}\n",
    "$$\n",
    "\n",
    "The worker chooses between accepting and rejecting in order to maximize expected lifetime value.\n",
    "\n",
    "### The Bellman equation\n",
    "\n",
    "The Bellman equation is\n",
    "\n",
    "$$\n",
    "    v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "The solution to this equation is called the **value function** and we denote it $v^*$.\n",
    "\n",
    "It is known that a policy is optimal if and only if \n",
    "\n",
    "$$\n",
    "    \\sigma(w) = \\mathbf 1 \n",
    "        \\left\\{\n",
    "            \\frac{w}{1-\\beta} \\geq c + \\beta \\sum_{w'} v^*(w') P(w, w')\n",
    "        \\right\\}\n",
    "$$\n",
    "\n",
    "Here the meaning of $\\sigma(w) = 1$ is \"stop\" (accept), while $\\sigma(w) = 0$ is \"continue\" (reject).\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "We solve this model using value function iteration.\n",
    "\n",
    "This means that we use the Bellman operator\n",
    "\n",
    "$$\n",
    "    (Tv)(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "The steps are\n",
    "\n",
    "1. pick an initial guess $v$\n",
    "2. iterate with $T$ to produce $v_k = T^k v$\n",
    "3. choose a $v_k$ **greedy** policy $\\sigma$, meaning that $\\sigma$ satisfies\n",
    "\n",
    "$$\n",
    "    \\sigma(w) = \\mathbf 1 \n",
    "        \\left\\{\n",
    "            \\frac{w}{1-\\beta} \\geq c + \\beta \\sum_{w'} v_k(w') P(w, w')\n",
    "        \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d862616",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Let's set up a namedtuple to store information needed to solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341948d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', \n",
    "                   ('n',        # wage grid size\n",
    "                    'w_vals',   # wage values \n",
    "                    'P',        # transition matrix\n",
    "                    'β',        # discount factor\n",
    "                    'c'))       # unemployment compensation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78fd5c3",
   "metadata": {},
   "source": [
    "The function below holds default values and populates the namedtuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0,       # unemployment compensation\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), jnp.array(mc.P)\n",
    "    return Model(n, w_vals, P, β, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f2d11",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15306602",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_js_model(β=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73274e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5dda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.w_vals.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ad312",
   "metadata": {},
   "source": [
    "Here's the Bellman operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9667f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T(v, model):\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β E v} with \n",
    "\n",
    "        e(w) = w / (1-β) and (Ev)(w) = E_w[ v(W')]\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c = model\n",
    "    h = c + β * P @ v\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af0161",
   "metadata": {},
   "source": [
    "The next function computes the optimal policy under the assumption that $v$ is\n",
    "                 the value function.\n",
    "\n",
    "The policy takes the form\n",
    "\n",
    "$$\n",
    "    \\sigma(w) = \\mathbf 1 \n",
    "        \\left\\{\n",
    "            \\frac{w}{1-\\beta} \\geq c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "        \\right\\}\n",
    "$$\n",
    "\n",
    "Here $\\mathbf 1$ is an indicator function.\n",
    "\n",
    "* $\\sigma(w) = 1$ means stop\n",
    "* $\\sigma(w) = 0$ means continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_greedy(v, model):\n",
    "    \"Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + β * P @ v\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa01b5",
   "metadata": {},
   "source": [
    "Here's a routine for value function iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfi(model, max_iter=10_000, tol=1e-4):\n",
    "    \"Solve the infinite-horizon Markov job search model by VFI.\"\n",
    "    print(\"Starting VFI iteration.\")\n",
    "    v = jnp.zeros_like(model.w_vals)    # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while error > tol and i < max_iter:\n",
    "        new_v = T(v, model)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        i += 1\n",
    "        v = new_v\n",
    "\n",
    "    v_star = v\n",
    "    σ_star = get_greedy(v_star, model)\n",
    "    return v_star, σ_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa42fad",
   "metadata": {},
   "source": [
    "## Computing the solution\n",
    "\n",
    "Let's set up and solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_js_model()\n",
    "n, w_vals, P, β, c = model\n",
    "\n",
    "v_star, σ_star = vfi(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0aa86",
   "metadata": {},
   "source": [
    "Here's the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b094daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(σ_star)\n",
    "ax.set_xlabel(\"wage values\")\n",
    "ax.set_ylabel(\"optimal choice (stop=1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473b882",
   "metadata": {},
   "source": [
    "We compute the reservation wage as the first $w$ such that $\\sigma(w)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937cb90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_indices = jnp.where(σ_star == 1)\n",
    "stop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wage_index = min(stop_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ad355",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wage = w_vals[res_wage_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b993d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star, alpha=0.8, label=\"value function\")\n",
    "ax.vlines((res_wage,), 150, 400, 'k', ls='--', label=\"reservation wage\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2aeb3d",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In the setting above, the agent is risk-neutral vis-a-vis future utility risk.\n",
    "\n",
    "Now solve the same problem but this time assuming that the agent has risk-sensitive\n",
    "preferences, which are a type of nonlinear recursive preferences.\n",
    "\n",
    "The Bellman equation becomes\n",
    "\n",
    "$$\n",
    "    v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, \n",
    "            c + \\frac{\\beta}{\\theta}\n",
    "            \\ln \\left[ \n",
    "                      \\sum_{w'} \\exp(\\theta v(w')) P(w, w')\n",
    "                \\right]\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "When $\\theta < 0$ the agent is risk averse.\n",
    "\n",
    "Solve the model when $\\theta = -0.1$ and compare your result to the risk neutral\n",
    "case.\n",
    "\n",
    "Try to interpret your result.\n",
    "\n",
    "You can start with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', ('n', 'w_vals', 'P', 'β', 'c', 'θ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_risk_sensitive_js_model(\n",
    "        n=500,       # wage grid size\n",
    "        ρ=0.9,       # wage persistence\n",
    "        ν=0.2,       # wage volatility\n",
    "        β=0.99,      # discount factor\n",
    "        c=1.0,       # unemployment compensation\n",
    "        θ=-0.1       # risk parameter\n",
    "    ):\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), mc.P\n",
    "    P = jnp.array(P)\n",
    "    return Model(n, w_vals, P, β, c, θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2b993",
   "metadata": {},
   "source": [
    "Now you need to modify `T` and `get_greedy` and then run value function iteration again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abe099",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(\"Solution below!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T_rs(v, model):\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β R v} with \n",
    "\n",
    "        e(w) = w / (1-β) and\n",
    "\n",
    "        (Rv)(w) = (1/θ) ln{E_w[ exp(θ v(W'))]}\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_greedy_rs(v, model):\n",
    "    \" Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ\n",
    "\n",
    "\n",
    "\n",
    "def vfi(model, max_iter=10_000, tol=1e-4):\n",
    "    \"Solve the infinite-horizon Markov job search model by VFI.\"\n",
    "    print(\"Starting VFI iteration.\")\n",
    "    v = jnp.zeros_like(model.w_vals)    # Initial guess\n",
    "    i = 0\n",
    "    error = tol + 1\n",
    "\n",
    "    while error > tol and i < max_iter:\n",
    "        new_v = T_rs(v, model)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        i += 1\n",
    "        v = new_v\n",
    "\n",
    "    v_star = v\n",
    "    σ_star = get_greedy_rs(v_star, model)\n",
    "    return v_star, σ_star\n",
    "\n",
    "\n",
    "\n",
    "model_rs = create_risk_sensitive_js_model()\n",
    "n, w_vals, P, β, c, θ = model_rs\n",
    "\n",
    "v_star_rs, σ_star_rs = vfi(model_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51990b41",
   "metadata": {},
   "source": [
    "Let's plot the results together with the original risk neutral case and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_indices = jnp.where(σ_star_rs == 1)\n",
    "res_wage_index = min(stop_indices[0])\n",
    "res_wage_rs = w_vals[res_wage_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star,  alpha=0.8, label=\"risk neutral $v$\")\n",
    "ax.plot(w_vals, v_star_rs, alpha=0.8, label=\"risk sensitive $v$\")\n",
    "ax.vlines((res_wage,), 100, 400,  ls='--', color='darkblue', \n",
    "          alpha=0.5, label=r\"risk neutral $\\bar w$\")\n",
    "ax.vlines((res_wage_rs,), 100, 400, ls='--', color='orange', \n",
    "          alpha=0.5, label=r\"risk sensitive $\\bar w$\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f026223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
